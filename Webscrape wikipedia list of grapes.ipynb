{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "website_url = requests.get('https://en.wikipedia.org/wiki/List_of_grape_varieties').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(website_url,'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_table = soup.find('table',{'class':'wikitable sortable'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = My_table.findAll('tr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "WIKI_URL = \"https://en.wikipedia.org/wiki/List_of_grape_varieties\"\n",
    "\n",
    "req = requests.get(WIKI_URL)\n",
    "soup = BeautifulSoup(req.content, 'lxml')\n",
    "table_classes = {\"class\": [\"sortable\", \"plainrowheaders\"]}\n",
    "wikitables = soup.findAll(\"table\", table_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going in a different direction to get more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import platform\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def scrape(url, output_name):\n",
    "    \"\"\"Create CSVs from all tables in a Wikipedia article.\n",
    "    ARGS:\n",
    "        url (str): The full URL of the Wikipedia article to scrape tables from.\n",
    "        output_name (str): The base file name (without filepath) to write to.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read tables from Wikipedia article into list of HTML strings\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content, 'lxml')\n",
    "    table_classes = {\"class\": [\"sortable\", \"plainrowheaders\"]}\n",
    "    wikitables = soup.findAll(\"table\", table_classes)\n",
    "\n",
    "    # Create folder for output if it doesn't exist\n",
    "    try:\n",
    "        os.mkdir(output_name)\n",
    "    except Exception:  # Generic OS Error\n",
    "        pass\n",
    "\n",
    "    for index, table in enumerate(wikitables):\n",
    "        # Make a unique file name for each CSV\n",
    "        if index == 0:\n",
    "            filename = output_name\n",
    "        else:\n",
    "            filename = output_name + '_' + str(index)\n",
    "\n",
    "        filepath = os.path.join(output_name, filename) + '.csv'\n",
    "\n",
    "        with open(filepath, mode='w', newline='', encoding='utf-8') as output:\n",
    "            # Deal with Windows inserting an extra '\\r' in line terminators\n",
    "            if platform.system() == 'Windows':\n",
    "                kwargs = {'lineterminator': '\\n'}\n",
    "\n",
    "                csv_writer = csv.writer(output,\n",
    "                                        quoting=csv.QUOTE_ALL,\n",
    "                                        **kwargs)\n",
    "            else:\n",
    "                csv_writer = csv.writer(output,\n",
    "                                        quoting=csv.QUOTE_ALL)\n",
    "\n",
    "            write_html_table_to_csv(table, csv_writer)\n",
    "\n",
    "\n",
    "def write_html_table_to_csv(table, writer):\n",
    "    \"\"\"Write HTML table from Wikipedia to a CSV file.\n",
    "    ARGS:\n",
    "        table (bs4.Tag): The bs4 Tag object being analyzed.\n",
    "        writer (csv.writer): The csv Writer object creating the output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hold elements that span multiple rows in a list of\n",
    "    # dictionaries that track 'rows_left' and 'value'\n",
    "    saved_rowspans = []\n",
    "    for row in table.findAll(\"tr\"):\n",
    "        cells = row.findAll([\"th\", \"td\"])\n",
    "\n",
    "        # If the first row, use it to define width of table\n",
    "        if len(saved_rowspans) == 0:\n",
    "            saved_rowspans = [None for _ in cells]\n",
    "        # Insert values from cells that span into this row\n",
    "        elif len(cells) != len(saved_rowspans):\n",
    "            for index, rowspan_data in enumerate(saved_rowspans):\n",
    "                if rowspan_data is not None:\n",
    "                    # Insert the data from previous row; decrement rows left\n",
    "                    value = rowspan_data['value']\n",
    "                    cells.insert(index, value)\n",
    "\n",
    "                    if saved_rowspans[index]['rows_left'] == 1:\n",
    "                        saved_rowspans[index] = None\n",
    "                    else:\n",
    "                        saved_rowspans[index]['rows_left'] -= 1\n",
    "\n",
    "        # If an element with rowspan, save it for future cells\n",
    "        for index, cell in enumerate(cells):\n",
    "            if cell.has_attr(\"rowspan\"):\n",
    "                rowspan_data = {\n",
    "                    'rows_left': int(cell[\"rowspan\"]),\n",
    "                    'value': cell,\n",
    "                }\n",
    "                saved_rowspans[index] = rowspan_data\n",
    "\n",
    "        if cells:\n",
    "            # Clean the data of references and unusual whitespace\n",
    "            cleaned = clean_data(cells)\n",
    "\n",
    "            # Fill the row with empty columns if some are missing\n",
    "            # (Some HTML tables leave final empty cells without a <td> tag)\n",
    "            columns_missing = len(saved_rowspans) - len(cleaned)\n",
    "            if columns_missing:\n",
    "                cleaned += [None] * columns_missing\n",
    "\n",
    "            writer.writerow(cleaned)\n",
    "\n",
    "\n",
    "def clean_data(row):\n",
    "    \"\"\"Clean table row list from Wikipedia into a string for CSV.\n",
    "    ARGS:\n",
    "        row (bs4.ResultSet): The bs4 result set being cleaned for output.\n",
    "    RETURNS:\n",
    "        cleaned_cells (list[str]): List of cleaned text items in this row.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_cells = []\n",
    "\n",
    "    for cell in row:\n",
    "        # Strip references from the cell\n",
    "        references = cell.findAll(\"sup\", {\"class\": \"reference\"})\n",
    "        if references:\n",
    "            for ref in references:\n",
    "                ref.extract()\n",
    "\n",
    "        # Strip sortkeys from the cell\n",
    "        sortkeys = cell.findAll(\"span\", {\"class\": \"sortkey\"})\n",
    "        if sortkeys:\n",
    "            for ref in sortkeys:\n",
    "                ref.extract()\n",
    "\n",
    "        # Strip footnotes from text and join into a single string\n",
    "        text_items = cell.findAll(text=True)\n",
    "        no_footnotes = [text for text in text_items if text[0] != '[']\n",
    "\n",
    "        cleaned = (\n",
    "            ''.join(no_footnotes)  # Combine elements into single string\n",
    "            .replace('\\xa0', ' ')  # Replace non-breaking spaces\n",
    "            .replace('\\n', ' ')  # Replace newlines\n",
    "            .strip()\n",
    "        )\n",
    "\n",
    "        cleaned_cells += [cleaned]\n",
    "\n",
    "    return cleaned_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape('https://en.wikipedia.org/wiki/List_of_grape_varieties', 'grapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://github.com/rocheio/wiki-table-scrape/blob/master/wikitablescrape.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
